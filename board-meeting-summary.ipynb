{"cells":[{"cell_type":"markdown","metadata":{"id":"uWkYWZzgDdPP"},"source":["Install dependenies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Y4pXjYaDj_s"},"outputs":[],"source":["!pip install requests bs4 whisper openai-whisper langchain langchain_openai"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pathlib import Path\n","\n","directory_list = [\"audio\", \"transcripts\", \"text\", \"summary\"]\n","\n","for directory in directory_list:\n","    Path(f\"/content/{directory}\").mkdir(parents=True, exist_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"xsSne3MuBpYu"},"source":["Get a list of meetings to download. Update the `meetings` list to add or remove a meeting type to download."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cRpSpxLO2_he"},"outputs":[],"source":["import requests, os\n","from bs4 import BeautifulSoup\n","\n","url = \"https://townhallstreams.com/towns/sunapee_nh\"\n","meetings = [\n","    \"Zoning Board Meeting\",\n","    \"Selectboard Meeting\",\n","    \"Planning Board Meeting\",\n","    \"Zoning_Board_Meeting\",\n","    \"Selectboard_Meeting\",\n","    \"Planning_Board_Meeting\"\n","]\n","\n","# The meeting links on the main page are .php with an id. This redirects\n","# to the actual video url. From there we can extract the m3u8 stream\n","def _get_meeting_urls_from_root_page(url):\n","    response = requests.get(url, verify=False)\n","    soup = BeautifulSoup(response.content, \"html.parser\")\n","    meeting_urls = []\n","\n","    # filter only relevant meetings\n","    for link in soup.find_all(\"a\"):\n","        if any(meeting in link.text for meeting in meetings):\n","            meeting_urls.append(link[\"href\"])\n","    return meeting_urls\n","\n","def _get_video_url_from(meeting_url):\n","    response = requests.get(f\"https://townhallstreams.com{meeting_url}\", verify=False)\n","    soup = BeautifulSoup(response.content, \"html.parser\")\n","\n","    for video in soup.find_all(\"script\", attrs={\"type\":\"text/javascript\"}):\n","        if len(video) != 0:\n","            if any(meeting in video.string for meeting in meetings):\n","                # extract the m3u8 link from the video tag\n","                m3u8_start_index = video.contents[0].string.find(\"https://\")\n","                m3u8_end_index = video.contents[0].string.find(\".m3u8\") + len(\".m3u8\")\n","                return video.contents[0].string[m3u8_start_index:m3u8_end_index]\n","\n","def get_list_of_video_urls(url):\n","    video_urls = []\n","    meeting_urls = _get_meeting_urls_from_root_page(url)\n","\n","    for meeting_url in meeting_urls:\n","        video_url = _get_video_url_from(meeting_url)\n","        if video_url is not None:\n","            video_urls.append(video_url)\n","    return video_urls\n","\n","\n","video_urls = get_list_of_video_urls(url)"]},{"cell_type":"markdown","metadata":{"id":"6lnTWCPzB9Qn"},"source":["Convert video files to `.mp3` and download."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nr10cSix4QQe"},"outputs":[],"source":["import shlex, subprocess, os\n","\n","audio_files = [file for file in os.listdir(\"/content/audio\") if file.endswith(\".mp3\")]\n","files_to_skip = []\n","\n","# get a list of audio_files that are in the video_urls\n","for video_url in video_urls:\n","    for audio_file in audio_files:\n","        if audio_file.split(\".mp3\")[0] in video_url:\n","            files_to_skip.append(video_url)\n","\n","# download any missing videos\n","for video_url in video_urls:\n","    if video_url not in files_to_skip:\n","        print(f\"Downloading {video_url}\")\n","        subprocess.run(shlex.split((f\"ffmpeg -n -i {video_url} /content/audio/{video_url.split('/')[-2].strip('.mp4')}.mp3\")))\n","    else:\n","        print(f\"Skipping {video_url}\")"]},{"cell_type":"markdown","metadata":{"id":"5sZLKIvOCY10"},"source":["Transcribe `.mp3` file. Output is a `.json` file which includes metatdata about the transcription such as the time series of text."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D5TVTKzN96tL"},"outputs":[],"source":["import whisper\n","import json\n","\n","audio_files = [file for file in os.listdir(\"/content/audio\") if file.endswith(\".mp3\")]\n","transcript_files = [file for file in os.listdir(\"/content/transcripts\") if file.endswith(\".json\")]\n","model = whisper.load_model('medium.en')\n","\n","# remove audio files that already have a transcript\n","for transcript_file in transcript_files:\n","    print(f\"Removing {transcript_file}\")\n","    audio_files.remove(f\"{transcript_file.strip('.json')}.mp3\")\n","\n","for audio_file in audio_files:\n","    print(f\"Transribing {audio_file}...\")\n","    result = model.transcribe(f\"/content/audio/{str(audio_file)}\", language='en', verbose=True)\n","    with open(f\"/content/transcripts/{audio_file.strip('.mp3')}.json\", \"w\") as file:\n","        json.dump(result, file, indent=4)"]},{"cell_type":"markdown","metadata":{"id":"y2_m9MiPBcNC"},"source":["Extract the text from the transcript `.json` file."]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":498,"status":"ok","timestamp":1710169089767,"user":{"displayName":"Brian McAllister","userId":"11126804884422281987"},"user_tz":240},"id":"2f5YKn1jAuA0"},"outputs":[],"source":["import json\n","\n","transcript_files = [file for file in os.listdir(\"/content/transcripts\") if file.endswith(\".json\")]\n","\n","for transcript_file in transcript_files:\n","    with open(f\"/content/text/{transcript_file.strip('.json')}.txt\", \"w\") as f:\n","        f.write(json.load(open(f\"/content/transcripts/{transcript_file}\"))[\"text\"])"]},{"cell_type":"markdown","metadata":{"id":"PbYlyP8hDGF-"},"source":["Create meeting summary. This requires an OpenAI API key.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":401},"executionInfo":{"elapsed":2815,"status":"error","timestamp":1710169732589,"user":{"displayName":"Brian McAllister","userId":"11126804884422281987"},"user_tz":240},"id":"UgTjvNGUDNQ1","outputId":"b4c61248-b469-4601-c7ae-59dad3527005"},"outputs":[],"source":["import os\n","\n","from langchain.prompts import PromptTemplate\n","from langchain.chains.summarize import load_summarize_chain\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_openai import ChatOpenAI\n","\n","\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", max_tokens=2000)\n","summary_chain = load_summarize_chain(llm=llm, chain_type='map_reduce')\n","\n","prompt_template = \"\"\"You are an AI assistant tasked with creating a detailed summary from the provided meeting transcription,\n","                  indicated with {text}.\\n\\n\\n\n","                  First, analyze the entire {text} to identify all cases discussed.\n","\"\"\"\n","\n","PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n","\n","refine_template = (\"\"\"For each case, provide detailed notes from the {text}. Focus on capturing\n","                   all the relevant details discussed during the meeting related to the individual cases, formatted\n","                   for easy readability. Include the case numbers, parcel ids, applicant names, special exceptions, variances, and article numbers. Avoid truncation or\n","                   summary but instead strive for as much detail as possible, articulated in grammatically correct English.\n","\"\"\"\n",")\n","refine_prompt = PromptTemplate(\n","    input_variables=[\"existing_answer\", \"text\"],\n","    template=refine_template,\n",")\n","chain = load_summarize_chain(\n","    llm,\n","    chain_type=\"map_reduce\",\n","    verbose=False,\n","    map_prompt=PROMPT,\n","    combine_prompt=refine_prompt\n",")\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=1000)\n","\n","text_files = [file for file in os.listdir(\"text\")]\n","summary_files = [file for file in os.listdir(\"summary\")]\n","\n","for summary_file in summary_files:\n","    print(f\"Removing {summary_file}\")\n","    text_files.remove(f\"{summary_file.strip('_Summary.txt')}.txt\")\n","\n","for file in text_files:\n","    print(f\"Processing {file}\")\n","    with open(f\"text/{file}\", \"r\") as f:\n","        raw_text = f.read()\n","\n","        docs = text_splitter.create_documents([raw_text])\n","        print(f\"Processing: {file}\")\n","        output = chain.invoke(docs)\n","        print(output[\"output_text\"])\n","        with open(f\"summary/{file.split('.txt')}_Summary.txt\", \"w\") as f:\n","            f.write(output[\"output_text\"])"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNBmD8GtFMCDYmwVDxiXdMB","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
